<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yanjiang Guo</title>

    <meta name="author" content="Yanjiang Guo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:top">
                <p class="name" style="text-align: center;">
                  Yanjiang Guo
                </p>
                <p> Hi! I am a 4th-year CS PhD student at <a href="https://iiis.tsinghua.edu.cn/"> Tsinghua University</a>, advised by Prof. <a href="https://people.iiis.tsinghua.edu.cn/~jychen/">Jianyu Chen</a>.
                Previously, I received my bachelor's degree in Electronic Engineering also from<a href="https://www.ee.tsinghua.edu.cn/"> Tsinghua University</a> in 2022.
                <p>
                <font color="red"><strong>Life Update: </strong></font>Currently, I am a visiting researcher at <a href="https://irislab.stanford.edu/">Stanford University</a> and fortunate to work with Prof. <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a> (Founder of <a href="https://www.physicalintelligence.company/">Physical Intelligence</a>).
                </p>

                <p> 
                My research focuses on <strong>Embodied AI and Generative Models</strong>, with a particular emphasis on <strong>training robot foundation models</strong> capable of performing a wide range of tasks in physical world. I prefer simple and scalable methods :)
                
                
             
                </p>
                <p style="text-align:center">
                  <a href="mailto:yanjiangguo666@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=rBeZZPMAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/GYanjiang">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Robert-gyj">Github</a>
                </p>
                
                <!-- <p>
                  <strong>My Quote:</strong>
                  
                  <em>"Set your course by the stars, not by the lights of passing ships."</em>
                </p> -->
                
                <p>
                  <strong>Events:</strong>
                  <br> Invited talks/visits at <a href="https://r-pad.github.io/">R-PAD Lab@CMU</a>, <a href="https://nicsefc.ee.tsinghua.edu.cn/">NICS-EFC Lab@Tsinghua</a>, <a href="https://www.dyna.co/">Dyna Robotics</a>...
                  <br>[2024.07] Best Paper Award Finalists in RSS 2024.
                  <br>[2022.06] Outstanding Graduates Award (Top 10% Tsinghua undergraduate students).
                  <br>[2017.11] Silver Medal in 34th National Physics Olympiad (CPhO).

                </p>

                <!-- <p>
                  <strong>Contact</strong>
                  <br>[2024.07] RSS 2024<font color="red"><strong> Best Paper Award Finalists</strong></font>.
                  <br>[2022.06] Outstanding Graduates of Tsinghua (Top 10%).
                  <br>[2017.11] 34th Chinese Physics Olympiad (CPhO), Silver Metal.
                </p> -->

              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <a href="images/gold_state_bridge.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/gold_state_bridge.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>



    <h2>Selected Research (* indicates equal contribution)</h2>

          
    <h3><span class="highlight"> Generative World Models:</span></h3>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='nuvo_image' style="width: 100%;"><video  width=100% muted autoplay loop>
          <source src="images/ctrl.jpg">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/ctrl.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://ctrl-world.github.io/">
          <span class="papertitle">Ctrl-World: A Controllable Generative World Model for Robot Manipulation</span>
        </a>
        <br>
        <strong>Yanjiang Guo*</strong>, Lucy Xiaoyang Shi*, Jianyu Chen, Chelsea Finn

        <br>
        <strong><em>Arxiv</em>, 2025</strong>
        <br>
        <a href="https://ctrl-world.github.io/">project page</a>
        /
        <a href="https://github.com/Robert-gyj/Ctrl-World">code</a>
        /
        <a href="https://ctrl-world.github.io/">arxiv</a>
        /
        <a href="https://x.com/GYanjiang/status/1981580263194034331">twitter</a>

        <p></p>
        We train a controllable generative world model that can be used to evaluate and improve SOTA generalist robot policy. 

      </td>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='nuvo_image' style="width: 100%;"><video  width=100% muted autoplay loop>
          <source src="images/vpp2.png">
          Your browser does not support the video tag.
          </video></div>
          <!-- <video src='images/doremi_clip.mp4' type="video/mp4" width=100%></video> -->
          <img src='images/vpp2.png' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://video-prediction-policy.github.io/">
          <span class="papertitle">Video Prediction Policy: A Generalist Robot Policy with Predictive Visual
            Representations</span>
        </a>
        <br>
        Yucheng Hu*, <strong>Yanjiang Guo*</strong>, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen

        <br>
        <strong><em>ICML</em>, 2025 </strong> &nbsp <font color="red"><strong>(Spotlight, 2.6%)</strong></font>
        <br>
        <a href="https://video-prediction-policy.github.io/">project page</a>
        /
        <a href="https://github.com/roboterax/video-prediction-policy">code</a>
        /
        <a href="https://arxiv.org/abs/2412.14803">arXiv</a>
        /
        <a href="https://x.com/GYanjiang/status/1871001766457971113">twitter</a>
        /
        <a href="https://mp.weixin.qq.com/s/JF6TLmHBguOCt3JyfAXVUg">机器之心</a>
        /
        <a href="https://mp.weixin.qq.com/s/oxIZWyat8hqtbgwCCGLu9A">量子位</a>
        <p></p>
        <!-- <span class="highlight">Video Generation (or World Model) based Robotic Foundation Model</span> -->
        <!-- <br> -->
        We finetune a general-purpose video diffusion model into manipulation-focused video prediction model to guide policy learning. 
          <!-- Notably, our generalist policy solves over 100+ dexterous hand manipulation tasks. -->

      </td>
    

    </tr>

        <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='nuvo_image' style="width: 100%;"><video  width=100% muted autoplay loop>
          <source src="images/PAD2.png">
          Your browser does not support the video tag.
          </video></div>
          <!-- <video src='images/doremi_clip.mp4' type="video/mp4" width=100%></video> -->
          <img src='images/PAD2.png' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://sites.google.com/view/pad-paper">
          <span class="papertitle">Prediction with Action: Visual Policy Learning via Joint Denoising Process</span>
        </a>
        <br>
		    <strong>Yanjiang Guo*</strong>, Yucheng Hu*, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu#, Jianyu Chen#
        <br>
        <strong><em>NeurIPS</em>, 2024</strong>
        <br>
        <a href="https://sites.google.com/view/pad-paper">project page</a>
        /
        <a href="https://github.com/Robert-gyj/Prediction_with_Action">code</a>
        /
        <a href="https://arxiv.org/pdf/2411.18179">arXiv</a>

        <p></p>
        <!-- <span class="highlight">Video Generation (or World Model) based Robotic Foundation Model</span> -->
        <p>
        We jointly predict future images and robot actions in a unified DiT network, transfering physical knowledge from internet video data to robots.
        </p>

      </td>
    </tr>

    </tbody></table>

    <h3><span class="highlight">Vison-Language-Action Models:</span></h3>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='nuvo_image' style="width: 100%;"><video  width=100% muted autoplay loop>
          <source src="images/upvla.jpg">
          Your browser does not support the video tag.
          </video></div>
          <!-- <video src='images/doremi_clip.mp4' type="video/mp4" width=100%></video> -->
          <img src='images/upvla.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://arxiv.org/abs/2501.18867">
          <span class="papertitle">UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent</span>
        </a>
        <br>
        Jianke Zhang*, <strong>Yanjiang Guo*</strong>, Yucheng Hu*, Xiaoyu Chen, Jianyu Chen

        <br>
        <strong><em>ICML</em>, 2025</strong>
        <br>
        <a href="https://arxiv.org/abs/2501.18867">arXiv</a>
        /
        <a href="https://github.com/CladernyJorn/UP-VLA">code</a>

        <p>
        <!-- <span class="highlight">VLA Model.</span> -->
        We incoperate both multi-modal understanding (MMU) and future prediction into VLA model, enhancing both high-level semantic knowledge and low-level visual dynamics.
        </p>
      </td>
    </tr>


    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='nuvo_image' style="width: 100%;"><video  width=100% muted autoplay loop>
          <source src="images/irevla.jpg">
          Your browser does not support the video tag.
          </video></div>
          <!-- <video src='images/doremi_clip.mp4' type="video/mp4" width=100%></video> -->
          <img src='images/irevla.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://arxiv.org/abs/2501.16664">
          <span class="papertitle">Improving Vision-Language-Action Model with Online Reinforcement Learning</span>
        </a>
        <br>
		    <strong>Yanjiang Guo*</strong>, Jianke Zhang*, Xiaoyu Chen*, Xiang Ji, Yen-Jen Wang, Yucheng Hu, Jianyu Chen
        <br>
        <strong><em>ICRA</em>, 2025</strong>
        <br>
        <a href="https://arxiv.org/abs/2501.16664">arXiv</a>
        /
        <a href="https://x.com/RoboReading/status/1884473082053599459">twitter1</a>
        /
        <a href="https://x.com/chris_j_paxton/status/1887525625042248018">twitter2</a>
        <p></p>
        <!-- <span class="highlight">VLA Model.</span> -->
        <p>
          We make some initial exploration on leveraging online RL to improve the VLA model! We notice that online RL for VLA can be extremely unstable and thus we adopted a iterative approach.
        </p>

      </td>
    </tr>

    <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='bog_image' style="width: 100%;"><video  width=100% muted autoplay loop>
            <source src="images/hirt.png">
            Your browser does not support the video tag.
            </video></div>
            <!-- <video src='images/doremi_clip.mp4' type="video/mp4" width=100%></video> -->
            <img src='images/hirt.png' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://arxiv.org/pdf/2410.05273">
          <span class="papertitle">HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers</span>
        </a>
        <br>
				Jianke Zhang*, <strong>Yanjiang Guo*</strong>, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, Jianyu Chen	
        <br>
        <strong><em>CoRL</em>, 2024</strong>
        <br>
        <a href="https://arxiv.org/pdf/2410.05273">arXiv</a>
        /
        <a href="https://x.com/GYanjiang/status/1892594610750300488">twitter</a>
        /
        <a href="https://mp.weixin.qq.com/s/kytaMLu_-6Ojlva_8N28Ng">机器之心</a>
        <p></p>
        <!-- <span class="highlight">VLA Model</span> -->
        <p>
        We finetune pretrained VLM into VLA models with hierarchical transformers, keeping the generalization ability but also much higher control frequency.
        </p>
      </td>
    </tr>

        <tr onmouseout="bog_stop()" onmouseover="bog_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='bog_image' style="width: 100%;"><video  width=100% muted autoplay loop>
            <source src="images/doremi2.png">
            Your browser does not support the video tag.
            </video></div>
            <!-- <video src='images/doremi_clip.mp4' type="video/mp4" width=100%></video> -->
            <img src='images/doremi2.png' width=100%>
        </div>
        <script type="text/javascript">
          function bog_start() {
            document.getElementById('bog_image').style.opacity = "1";
          }

          function bog_stop() {
            document.getElementById('bog_image').style.opacity = "0";
          }
          bog_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://sites.google.com/view/doremi-paper">
          <span class="papertitle">DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment</span>
        </a>
        <br>
        <strong>Yanjiang Guo*</strong> , Yen-Jen Wang*, Lihan Zha*, Jianyu Chen
        <br>
        <strong><em>IROS</em>, 2024</strong>
        <br>
        <a href="https://sites.google.com/view/doremi-paper">project page</a>
        /
        <a href="https://arxiv.org/pdf/2307.00329">arXiv</a>
        <p></p>
        <p>
          We leverage LLM to pefrom both planning and monitoring, with a fine-tuned VLM as detector.
        </p>
      </td>
    </tr>

    <!-- <tr onmouseout="internerf_stop()" onmouseover="internerf_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='internerf_image' style="width: 100%;"><video  width=100% muted autoplay loop>
            <source src="images/doremi_clip.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/doremi.png' width=100%>
        </div>
        <script type="text/javascript">
          function internerf_start() {
            document.getElementById('internerf_image').style.opacity = "1";
          }

          function internerf_stop() {
            document.getElementById('internerf_image').style.opacity = "0";
          }
          internerf_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://sites.google.com/view/doremi-paper">
          <span class="papertitle">DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment</span>
        </a>
        <br>
        <strong>Yanjiang Guo*</strong> , Yen-Jen Wang*, Lihan Zha*, Jianyu Chen
        <br>
        <strong><em>IROS</em>, 2024</strong>
        <br>
        <a href="https://sites.google.com/view/doremi-paper">project page</a>
        /
        <a href="https://arxiv.org/pdf/2307.00329">arXiv</a>
        <p></p>
        <p>
          We leverage LLM to pefrom both planning and monitoring, with a fine-tuned VLM as detector.
        </p>
      </td>
    </tr> -->

    </tbody></table>


    <h3><span class="highlight">Reinforcement Learning:</span></h3>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
    
    <tr onmouseout="smerf_stop()" onmouseover="smerf_start()">
      <td style="padding:10px;width:35%;vertical-align:top">
        <div class="one" style="width: 100%;">
          <div class="two" id='smerf_image' style="width: 100%;"><video  width=100% muted autoplay loop>
            <source src="images/DWL.png">
            Your browser does not support the video tag.
            </video></div>
            <!-- <video src='images/doremi_clip.mp4' type="video/mp4" width=100%></video> -->
            <img src='images/DWL.png' width=100%>
        </div>
        <script type="text/javascript">
          function smerf_start() {
            document.getElementById('smerf_image').style.opacity = "1";
          }

          function smerf_stop() {
            document.getElementById('smerf_image').style.opacity = "0";
          }
          smerf_stop()
        </script>
      </td>
      <td style="padding:10px;width:65%;vertical-align:top">
        <a href="https://sites.google.com/view/humanoid-gym/">
          <span class="papertitle">Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning</span>
        </a>
        <br>
        Xinyang Gu*, Yen-Jen Wang*, Xiang Zhu*, Chengming Shi*, <strong>Yanjiang Guo</strong>, Yichen Liu, Jianyu Chen
        <br>
        <strong><em>RSS</em>, 2024</strong> &nbsp <font color="red"><strong>(Best Paper Award Finalists)</strong></font>
        <br>
        <a href="https://sites.google.com/view/humanoid-gym/">project page</a>
        /
        <a href="https://github.com/roboterax/humanoid-gym">code</a>
        /
        <a href="https://enriquecoronadozu.github.io/rssproceedings2024/rss20/p058.pdf">arXiv</a>
        /
        <a href="https://mp.weixin.qq.com/s/5lY5Fj82lrRWikn6LNceWg">机器之心</a>
        <p></p>
        <p>
        We train humanoid robot to master challenging terrains such as stairs, slopes, and snow grounds with zero-shot sim2real transfer.
        </p>
      </td>
    </tr>


    </tbody></table>

          <p></p>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Other Publications</h2>
                <p>
                  <a href="https://arxiv.org/pdf/2306.17411">
                    <span class="papertitle">Decentralized Motor Skill Learning for Complex Robotic Systems</span>
                  </a>
                  <br><strong>Yanjiang Guo*</strong>, Zheyuan Jiang*, Yen-Jen Wang, Jingyue Gao, Jianyu Chen
                  <br><em><strong>RA-L, 2023 (with ICRA 2024) </strong></em>
                  <p></p>
                  <a href="hhttps://arxiv.org/pdf/2210.00350">
                    <span class="papertitle">Zero-shot policy transfer with disentangled task representation of meta-reinforcement learning</span>
                  </a>
                  <br>Zheng Wu, Yichen Xie, Wenzhao Lian, Changhao Wang, <strong>Yanjiang Guo</strong>, Jianyu Chen, Stefan Schaal, Masayoshi Tomizuka
                  <br><em><strong>ICRA, 2023</strong></em>
                  <p></p>
                  <a href="https://proceedings.mlr.press/v205/guo23a/guo23a.pdf"> 
                    <span class="papertitle">Reinforcement learning with Demonstrations from Mismatched Task under Sparse Reward</span>
                  </a>
                  <br><strong>Yanjiang Guo</strong>, Jingyue Gao, Zheng Wu, Chengming Shi, Jianyu Chen
                  <br><em><strong>CoRL, 2022</strong></em>
                  <p></p>
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Honors and Awards</h2>
                <p>
                  <br>RSS Outstanding Paper Award Finalists, 2024.
                  <br>Outstanding Graduates of Tsinghua (Top 10%), 2022.
                  <br>34th Chinese Physics Olympiad (CPhO), Silver Metal, 2017.
                </p>
              </td>
            </tr>
          </tbody></table> -->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
